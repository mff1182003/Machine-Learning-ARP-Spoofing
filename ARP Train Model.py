import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, validation_curve
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import joblib
import time
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter


def add_noise_to_features(X, noise_factor=0.1, random_state=42):
    """ThÃªm nhiá»…u Gaussian vÃ o features sá»‘ - CHá»ˆ cho features thá»±c sá»± numerical"""
    np.random.seed(random_state)
    X_noisy = X.copy()

    # Danh sÃ¡ch features thá»±c sá»± lÃ  sá»‘
    numerical_indicators = ['Length', 'Delta time', 'delta_time', 'length_', '_x_', 'zscore']

    noise_applied = []
    for col in X_noisy.columns:
        # Kiá»ƒm tra xem cÃ³ pháº£i numerical feature khÃ´ng
        is_numerical = any(indicator in col for indicator in numerical_indicators)
        # Kiá»ƒm tra variance > 0
        has_variance = X_noisy[col].std() > 1e-6

        if is_numerical and has_variance:
            noise = np.random.normal(0, noise_factor * X_noisy[col].std(), size=len(X_noisy))
            X_noisy[col] = X_noisy[col] + noise
            noise_applied.append(col)

    print(f"ðŸŽ² ÄÃ£ thÃªm nhiá»…u cho {len(noise_applied)} features: {noise_applied}")
    return X_noisy


def create_advanced_features(df):
    """Táº¡o features CHá»ˆ tá»« dá»¯ liá»‡u cÃ³ sáºµn - khÃ´ng táº¡o features phá»©c táº¡p"""
    df = df.copy()

    # Chá»‰ táº¡o features Ä‘Æ¡n giáº£n tá»« dá»¯ liá»‡u cÃ³ sáºµn
    available_features = []

    # Features thá»i gian - chá»‰ náº¿u cÃ³ Delta time
    if 'Delta time' in df.columns:
        # Log transform cho delta time Ä‘á»ƒ giáº£m skewness
        df['delta_time_log'] = np.log1p(df['Delta time'].clip(lower=0))
        available_features.append('delta_time_log')

        print(f"   âœ… delta_time_log: min={df['delta_time_log'].min():.4f}, max={df['delta_time_log'].max():.4f}")

    # Features vá» packet length
    if 'Length' in df.columns:
        length_mean = df['Length'].mean()
        length_std = df['Length'].std()
        if length_std > 0:
            df['length_zscore'] = (df['Length'] - length_mean) / length_std
            available_features.append('length_zscore')
            print(f"   âœ… length_zscore: min={df['length_zscore'].min():.4f}, max={df['length_zscore'].max():.4f}")

        # Binary indicators cho packet size
        q25 = df['Length'].quantile(0.25)
        q75 = df['Length'].quantile(0.75)
        df['is_small_packet'] = (df['Length'] <= q25).astype(int)
        df['is_large_packet'] = (df['Length'] >= q75).astype(int)
        available_features.extend(['is_small_packet', 'is_large_packet'])
        print(f"   âœ… packet_size_indicators: small={df['is_small_packet'].sum()}, large={df['is_large_packet'].sum()}")

    # Interaction features - chá»‰ táº¡o nhá»¯ng cÃ¡i Ä‘Æ¡n giáº£n
    if 'Delta time' in df.columns and 'arp_type_numeric' in df.columns:
        df['arp_type_x_delta'] = df['arp_type_numeric'] * df['Delta time']
        available_features.append('arp_type_x_delta')
        print(f"   âœ… arp_type_x_delta: created")

    print(f"ðŸ”§ ÄÃ£ táº¡o {len(available_features)} advanced features: {available_features}")
    return df


def plot_learning_curves(pipeline, X, y, cv_folds=5):
    """Váº½ learning curves Ä‘á»ƒ phÃ¡t hiá»‡n overfitting"""
    from sklearn.model_selection import learning_curve

    train_sizes, train_scores, val_scores = learning_curve(
        pipeline, X, y, cv=cv_folds, n_jobs=-1,
        train_sizes=np.linspace(0.1, 1.0, 10),
        scoring='f1_macro'
    )

    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Training Score')
    plt.plot(train_sizes, np.mean(val_scores, axis=1), 'o-', label='Validation Score')
    plt.fill_between(train_sizes, np.mean(train_scores, axis=1) - np.std(train_scores, axis=1),
                     np.mean(train_scores, axis=1) + np.std(train_scores, axis=1), alpha=0.3)
    plt.fill_between(train_sizes, np.mean(val_scores, axis=1) - np.std(val_scores, axis=1),
                     np.mean(val_scores, axis=1) + np.std(val_scores, axis=1), alpha=0.3)

    plt.xlabel('Training Set Size')
    plt.ylabel('F1 Score')
    plt.title('Learning Curves')
    plt.legend()
    plt.grid(True)
    plt.show()

    return train_scores, val_scores


def train_arp_detection_model_robust(labeled_csv_file, test_size=0.2, cv_folds=10,
                                     noise_factor=0.15, random_state=42):
    """
    PhiÃªn báº£n cáº£i tiáº¿n vá»›i ká»¹ thuáº­t chá»‘ng overfitting
    """
    try:
        df = pd.read_csv(labeled_csv_file)
        print(f"ðŸ“Š ÄÃ£ Ä‘á»c {len(df)} máº«u dá»¯ liá»‡u")
    except Exception as e:
        print(f"âŒ Lá»—i khi Ä‘á»c file CSV: {e}")
        return None, None, None

    if 'Label' not in df.columns:
        print("âŒ Lá»—i: Thiáº¿u cá»™t 'Label'")
        print(f"CÃ¡c cá»™t cÃ³ sáºµn: {df.columns.tolist()}")
        return None, None, None

    # PhÃ¢n tÃ­ch phÃ¢n bá»‘ class
    print(f"ðŸ“ˆ PhÃ¢n bá»‘ nhÃ£n: {dict(df['Label'].value_counts())}")

    # In ra tÃªn cÃ¡c cá»™t Ä‘á»ƒ debug
    print(f"ðŸ” CÃ¡c cá»™t trong file: {df.columns.tolist()}")

    # Clean column names - loáº¡i bá» khoáº£ng tráº¯ng thá»«a
    df.columns = df.columns.str.strip()

    # Mapping tÃªn cá»™t tá»« CSV thá»±c táº¿
    column_mapping = {
        'No.': 'No',
        'Delta time': 'Delta time',
        'Frame length': 'Frame length'
    }

    # Rename náº¿u cáº§n
    for old_name, new_name in column_mapping.items():
        if old_name in df.columns:
            df.rename(columns={old_name: new_name}, inplace=True)

    # Kiá»ƒm tra cÃ¡c cá»™t cáº§n thiáº¿t
    required_columns = ['Info', 'Length', 'Label']
    optional_columns = ['Delta time', 'Source', 'Destination']

    missing_required = [col for col in required_columns if col not in df.columns]
    if missing_required:
        print(f"âŒ Thiáº¿u cÃ¡c cá»™t báº¯t buá»™c: {missing_required}")
        return None, None, None

    available_optional = [col for col in optional_columns if col in df.columns]
    print(f"âœ… CÃ¡c cá»™t tÃ¹y chá»n cÃ³ sáºµn: {available_optional}")

    # Táº¡o Delta time náº¿u khÃ´ng cÃ³
    if 'Delta time' not in df.columns:
        print("âš ï¸  KhÃ´ng cÃ³ cá»™t 'Delta time', táº¡o tá»« index")
        df['Delta time'] = df.index * 0.001  # Giáº£ sá»­ 1ms interval

    # Loáº¡i bá» cá»™t khÃ´ng cáº§n thiáº¿t (chá»‰ nhá»¯ng cá»™t thá»±c sá»± cÃ³)
    columns_to_drop = ['No', 'Time', 'Protocol', 'Tcp Flags', 'Time to live', 'Flags']
    df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)

    # Táº¡o Ä‘áº·c trÆ°ng cÆ¡ báº£n CHá»ˆ tá»« nhá»¯ng gÃ¬ cÃ³ sáºµn
    print("ðŸ”§ Táº¡o features cÆ¡ báº£n...")

    # ARP type tá»« Info
    df['arp_type_numeric'] = df['Info'].apply(lambda x: 1 if 'is at' in str(x) else 0 if 'Who has' in str(x) else -1)
    df = df[df['arp_type_numeric'] != -1].copy()
    print(f"   âœ… arp_type_numeric: {df['arp_type_numeric'].value_counts().to_dict()}")

    # Broadcast detection tá»« Info
    df['is_broadcast'] = df['Info'].apply(lambda x: 1 if 'Broadcast' in str(x) else 0)
    print(f"   âœ… is_broadcast: {df['is_broadcast'].value_counts().to_dict()}")

    # Source-Destination analysis náº¿u cÃ³
    if 'Source' in df.columns and 'Destination' in df.columns:
        df['same_src_dst'] = (df['Source'] == df['Destination']).astype(int)
        print(f"   âœ… same_src_dst: {df['same_src_dst'].value_counts().to_dict()}")
    else:
        print("   âš ï¸  KhÃ´ng cÃ³ cá»™t Source/Destination")

    # Táº¡o features nÃ¢ng cao
    df = create_advanced_features(df)

    # Chá»n features CHá»ˆ tá»« nhá»¯ng gÃ¬ thá»±c sá»± cÃ³ trong DataFrame
    base_features = ['Length', 'arp_type_numeric']
    if 'Delta time' in df.columns:
        base_features.append('Delta time')

    # Optional features
    optional_features = ['is_broadcast']
    if 'same_src_dst' in df.columns:
        optional_features.append('same_src_dst')

    # Advanced features (chá»‰ nhá»¯ng cÃ¡i Ä‘Æ°á»£c táº¡o thÃ nh cÃ´ng)
    possible_advanced = ['delta_time_log', 'length_zscore', 'is_small_packet',
                         'is_large_packet', 'arp_type_x_delta']

    # Tá»•ng há»£p táº¥t cáº£ features cÃ³ thá»ƒ
    all_possible_features = base_features + optional_features + possible_advanced

    # CHá»ˆ sá»­ dá»¥ng features thá»±c sá»± cÃ³ trong DataFrame
    features = [f for f in all_possible_features if f in df.columns]

    print(f"ðŸ”§ Sá»¬ Dá»¤NG {len(features)} FEATURES:")
    for i, feature in enumerate(features, 1):
        print(f"   {i:2d}. {feature}")

    # Kiá»ƒm tra data quality cho tá»«ng feature
    print(f"\nðŸ“Š THá»NG KÃŠ FEATURES:")
    for feature in features:
        if df[feature].dtype in ['int64', 'float64']:
            print(
                f"   {feature:20s}: min={df[feature].min():8.4f}, max={df[feature].max():8.4f}, mean={df[feature].mean():8.4f}")
        else:
            print(f"   {feature:20s}: unique_values={df[feature].nunique()}")

    X = df[features]
    y = df['Label']

    print(f"\nðŸ“¦ FINAL DATASET: {X.shape[0]} samples x {X.shape[1]} features")

    # ThÃªm nhiá»…u Ä‘á»ƒ tÄƒng tÃ­nh robust
    print(f"ðŸŽ² ThÃªm nhiá»…u vá»›i há»‡ sá»‘ {noise_factor}")
    X_noisy = add_noise_to_features(X, noise_factor=noise_factor, random_state=random_state)

    # TÃ¡ch dá»¯ liá»‡u vá»›i stratify
    X_train, X_test, y_train, y_test = train_test_split(
        X_noisy, y, test_size=test_size, random_state=random_state, stratify=y
    )

    print(f"ðŸ“¦ Táº­p huáº¥n luyá»‡n: {len(X_train)} | Táº­p kiá»ƒm tra: {len(X_test)}")

    # TÃ¡ch features cho preprocessing - CHá»ˆ dá»±a trÃªn tÃªn thá»±c táº¿
    numerical_features = []
    binary_features = []

    for feature in features:
        # Numerical: Length, Delta time, log transforms, z-scores, interactions
        if any(keyword in feature.lower() for keyword in ['length', 'delta', 'log', 'zscore', '_x_']):
            numerical_features.append(feature)
        # Binary: is_, arp_type_numeric (0/1), same_
        elif any(keyword in feature.lower() for keyword in ['is_', 'arp_type_numeric', 'same_']):
            binary_features.append(feature)
        else:
            # Default to numerical for safety
            numerical_features.append(feature)

    print(f"ðŸ“Š PREPROCESSING SETUP:")
    print(f"   Numerical features ({len(numerical_features)}): {numerical_features}")
    print(f"   Binary features ({len(binary_features)}): {binary_features}")

    # Táº¡o preprocessing pipeline
    if binary_features:
        preprocessing = ColumnTransformer([
            ('num', StandardScaler(), numerical_features),
            ('bin', 'passthrough', binary_features)
        ])
    else:
        preprocessing = ColumnTransformer([
            ('num', StandardScaler(), numerical_features)
        ])
        print("âš ï¸  Chá»‰ cÃ³ numerical features, khÃ´ng cÃ³ binary features")

    # Pipeline vá»›i regularization máº¡nh hÆ¡n
    pipeline = Pipeline([
        ('preprocessing', preprocessing),
        ('clf', RandomForestClassifier(
            n_estimators=100,  # Giáº£m sá»‘ cÃ¢y
            max_depth=5,  # Giá»›i háº¡n Ä‘á»™ sÃ¢u
            min_samples_split=20,  # TÄƒng min samples Ä‘á»ƒ split
            min_samples_leaf=10,  # TÄƒng min samples á»Ÿ leaf
            max_features='sqrt',  # Giáº£m sá»‘ features má»—i split
            random_state=random_state,
            class_weight='balanced'  # CÃ¢n báº±ng class
        ))
    ])

    # Cross-validation chi tiáº¿t vá»›i nhiá»u fold
    print(f"ðŸ”„ Thá»±c hiá»‡n {cv_folds}-fold cross validation...")
    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)

    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=skf, scoring='f1_macro', n_jobs=-1)

    print(f"ðŸ“Š CV F1 Scores: {cv_scores}")
    print(f"ðŸ“Š Mean CV F1: {cv_scores.mean():.4f} (Â±{cv_scores.std() * 2:.4f})")

    # Validation curve Ä‘á»ƒ kiá»ƒm tra overfitting
    print("ðŸ“ˆ PhÃ¢n tÃ­ch validation curve...")
    param_range = [3, 5, 7, 10, 15, 20]
    train_scores, val_scores = validation_curve(
        pipeline, X_train, y_train, param_name='clf__max_depth',
        param_range=param_range, cv=5, scoring='f1_macro', n_jobs=-1
    )

    # TÃ¬m max_depth tá»‘i Æ°u
    val_mean = np.mean(val_scores, axis=1)
    optimal_depth = param_range[np.argmax(val_mean)]
    print(f"ðŸ† Optimal max_depth: {optimal_depth}")

    # Set optimal depth vÃ  train final model
    pipeline.set_params(clf__max_depth=optimal_depth)

    start_time = time.time()
    pipeline.fit(X_train, y_train)
    training_time = time.time() - start_time

    print(f"âœ… Huáº¥n luyá»‡n hoÃ n táº¥t sau {training_time:.2f} giÃ¢y")

    # ÄÃ¡nh giÃ¡ chi tiáº¿t
    y_train_pred = pipeline.predict(X_train)
    y_test_pred = pipeline.predict(X_test)

    train_acc = accuracy_score(y_train, y_train_pred)
    test_acc = accuracy_score(y_test, y_test_pred)
    train_f1 = f1_score(y_train, y_train_pred, average='macro')
    test_f1 = f1_score(y_test, y_test_pred, average='macro')

    print(f"\nðŸŽ¯ Káº¾T QUáº¢ ÄÃNH GIÃ:")
    print(f"   Training Accuracy: {train_acc:.4f}")
    print(f"   Test Accuracy: {test_acc:.4f}")
    print(f"   Training F1: {train_f1:.4f}")
    print(f"   Test F1: {test_f1:.4f}")
    print(f"   Accuracy Gap: {train_acc - test_acc:.4f}")
    print(f"   F1 Gap: {train_f1 - test_f1:.4f}")

    if (train_acc - test_acc) > 0.1 or (train_f1 - test_f1) > 0.1:
        print("âš ï¸  Cáº¢NH BÃO: CÃ³ dáº¥u hiá»‡u overfitting!")
    else:
        print("âœ… Model cÃ³ váº» á»•n Ä‘á»‹nh, khÃ´ng overfitting nghiÃªm trá»ng")

    print(f"\nðŸ“‹ Classification Report (Test Set):")
    print(classification_report(y_test, y_test_pred))

    print("\nðŸ§® Confusion Matrix (Test Set):")
    cm = confusion_matrix(y_test, y_test_pred)
    print(cm)

    # Feature importance
    if hasattr(pipeline.named_steps['clf'], 'feature_importances_'):
        importances = pipeline.named_steps['clf'].feature_importances_
        feature_imp = pd.DataFrame({
            'feature': features,
            'importance': importances
        }).sort_values('importance', ascending=False)

        print(f"\nðŸ” TOP 5 FEATURES QUAN TRá»ŒNG:")
        print(feature_imp.head())

    # LÆ°u mÃ´ hÃ¬nh
    joblib.dump(pipeline, 'arp_spoofing_model_robust.joblib')
    print("âœ… ÄÃ£ lÆ°u mÃ´ hÃ¬nh: arp_spoofing_model_robust.joblib")

    # Váº½ learning curves (náº¿u cÃ³ matplotlib)
    try:
        plot_learning_curves(pipeline, X_train, y_train, cv_folds=5)
    except:
        print("ðŸ“Š KhÃ´ng thá»ƒ váº½ learning curves (thiáº¿u matplotlib)")

    results = {
        'cv_scores': cv_scores,
        'train_accuracy': train_acc,
        'test_accuracy': test_acc,
        'train_f1': train_f1,
        'test_f1': test_f1,
        'optimal_depth': optimal_depth,
        'feature_importance': feature_imp if 'feature_imp' in locals() else None
    }

    return pipeline, results


# Cháº¡y vá»›i cáº¥u hÃ¬nh chá»‘ng overfitting
if __name__ == "__main__":
    print("ðŸš€ Báº¯t Ä‘áº§u training vá»›i ká»¹ thuáº­t chá»‘ng overfitting...")

    model, results = train_arp_detection_model_robust(
        'arp.csv',
        test_size=0.2,  # Giáº£m test size Ä‘á»ƒ cÃ³ nhiá»u data train hÆ¡n
        cv_folds=10,  # TÄƒng sá»‘ fold
        noise_factor=0.15,  # ThÃªm nhiá»…u 15%
        random_state=42
    )

    if results:
        print(f"\nðŸ“Š Tá»”NG Káº¾T:")
        print(f"CV Mean F1: {results['cv_scores'].mean():.4f}")
        print(f"Test F1: {results['test_f1']:.4f}")
        print(
            f"Overfitting Check: {'âŒ CÃ³ overfitting' if results['train_f1'] - results['test_f1'] > 0.1 else 'âœ… á»”n Ä‘á»‹nh'}")